{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GxrM36HZnZsE","executionInfo":{"status":"ok","timestamp":1653467783218,"user_tz":-330,"elapsed":757,"user":{"displayName":"Himanshu Nigam","userId":"07566813038449216225"}},"outputId":"0b6cb159-889c-4db5-ef16-84b87b97f3e9"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[33mWARNING: Skipping tweepy as it is not installed.\u001b[0m\n"]}],"source":["!pip uninstall tweepy"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28376,"status":"ok","timestamp":1653467811586,"user":{"displayName":"Himanshu Nigam","userId":"07566813038449216225"},"user_tz":-330},"id":"e5o6xxv5nfIz","outputId":"7fa594bf-3a07-4bdd-b92c-fc1442180837"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tweepy\n","  Downloading tweepy-4.10.0-py3-none-any.whl (94 kB)\n","\u001b[K     |████████████████████████████████| 94 kB 3.1 MB/s \n","\u001b[?25hRequirement already satisfied: oauthlib<4,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from tweepy) (3.2.0)\n","Collecting requests<3,>=2.27.0\n","  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n","\u001b[K     |████████████████████████████████| 63 kB 2.1 MB/s \n","\u001b[?25hRequirement already satisfied: requests-oauthlib<2,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from tweepy) (1.3.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.27.0->tweepy) (2022.5.18.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.27.0->tweepy) (1.24.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.27.0->tweepy) (2.10)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.27.0->tweepy) (2.0.12)\n","Installing collected packages: requests, tweepy\n","  Attempting uninstall: requests\n","    Found existing installation: requests 2.23.0\n","    Uninstalling requests-2.23.0:\n","      Successfully uninstalled requests-2.23.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n","Successfully installed requests-2.27.1 tweepy-4.10.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n","\u001b[K     |████████████████████████████████| 4.2 MB 12.8 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n","\u001b[K     |████████████████████████████████| 86 kB 3.5 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 47.0 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.27.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 48.2 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.0.12)\n","Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.7.0 pyyaml-6.0 tokenizers-0.12.1 transformers-4.19.2\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting emot\n","  Downloading emot-3.1-py3-none-any.whl (61 kB)\n","\u001b[K     |████████████████████████████████| 61 kB 19 kB/s \n","\u001b[?25hInstalling collected packages: emot\n","Successfully installed emot-3.1\n"]}],"source":["!pip install tweepy\n","!pip install nltk\n","!pip install transformers\n","!pip install emot"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"vtNhhvWomvWM","executionInfo":{"status":"ok","timestamp":1653467811587,"user_tz":-330,"elapsed":19,"user":{"displayName":"Himanshu Nigam","userId":"07566813038449216225"}}},"outputs":[],"source":["\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"qWfA5uKnoGf7","executionInfo":{"status":"ok","timestamp":1653467811588,"user_tz":-330,"elapsed":18,"user":{"displayName":"Himanshu Nigam","userId":"07566813038449216225"}}},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":4,"metadata":{"id":"NmKlC0YA8dYb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653467813456,"user_tz":-330,"elapsed":1885,"user":{"displayName":"Himanshu Nigam","userId":"07566813038449216225"}},"outputId":"c0b7d1e6-b41e-43cf-c104-004884f2b5f2"},"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/omw-1.4.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":4}],"source":["import nltk\n","nltk.download('stopwords')\n","nltk.download('omw-1.4')\n","nltk.download('punkt')"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"u5jLgBoa9h4X","executionInfo":{"status":"ok","timestamp":1653467813457,"user_tz":-330,"elapsed":6,"user":{"displayName":"Himanshu Nigam","userId":"07566813038449216225"}}},"outputs":[],"source":["import tweepy"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"GxlxFyx9_gW2","executionInfo":{"status":"ok","timestamp":1653467813457,"user_tz":-330,"elapsed":5,"user":{"displayName":"Himanshu Nigam","userId":"07566813038449216225"}}},"outputs":[],"source":["from tweepy import Client"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"117effGT8dYe","executionInfo":{"status":"ok","timestamp":1653467820536,"user_tz":-330,"elapsed":7084,"user":{"displayName":"Himanshu Nigam","userId":"07566813038449216225"}}},"outputs":[],"source":["\n","import pandas as pd\n","import re\n","import matplotlib.pyplot as plt\n","from wordcloud import WordCloud\n","%matplotlib inline\n","from transformers import pipeline, TFAutoModelForSequenceClassification , AutoTokenizer\n","from textblob import TextBlob\n","import re\n","import string # Inbuilt string library\n","\n","# Natural Language Processing Toolkit\n","from nltk.corpus import stopwords, words # get stopwords from NLTK library & get all words in english language\n","from nltk.tokenize import word_tokenize # to create word tokens\n","# from nltk.stem import PorterStemmer (I played around with Stemmer and decided to use Lemmatizer instead)\n","from nltk.stem import WordNetLemmatizer # to reduce words to orginal form\n","from nltk import pos_tag # For Parts of Speech tagging\n","\n","from emot.emo_unicode import UNICODE_EMOJI # For emojis\n","from emot.emo_unicode import EMOTICONS_EMO # For EMOTICONS"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"30N2UB-boNe0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653467820536,"user_tz":-330,"elapsed":15,"user":{"displayName":"Himanshu Nigam","userId":"07566813038449216225"}},"outputId":"8698582a-24b6-400e-c8b3-a07517ee113d"},"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package words to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/words.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":8}],"source":["nltk.download('words')"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"OB46wPpm8dYf","executionInfo":{"status":"ok","timestamp":1653467820537,"user_tz":-330,"elapsed":10,"user":{"displayName":"Himanshu Nigam","userId":"07566813038449216225"}}},"outputs":[],"source":["    num_of_pages = 3\n","    # Defining my NLTK stop words and my user-defined stop words\n","    stop_words = list(stopwords.words('english'))\n","    user_stop_words = ['2022', 'year', 'many', 'much', 'amp', 'next', 'cant', 'wont', 'hadnt',\n","                        'havent', 'hasnt', 'isnt', 'shouldnt', 'couldnt', 'wasnt', 'werent',\n","                        'mustnt', '’', '...', '..', '.', '.....', '....', 'been…', 'one', 'two',\n","                        'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten', 'aht',\n","                        've', 'next']\n","    alphabets = list(string.ascii_lowercase)\n","    stop_words = stop_words + user_stop_words + alphabets\n","    word_list = words.words()  # all words in English language\n","    emojis = list(UNICODE_EMOJI.keys())  # full list of emojis"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"jaJ7Qlrb8dYg","scrolled":false,"executionInfo":{"status":"ok","timestamp":1653467820537,"user_tz":-330,"elapsed":8,"user":{"displayName":"Himanshu Nigam","userId":"07566813038449216225"}}},"outputs":[],"source":["#getting recent tweets data\n","def getTweets():\n","    #asking for the search term and the desired number of tweets\n","    global keyword \n","    keyword = 'VikramTrailer OR #ProofOfInspiration1 OR #Cricket OR #IPL2022 OR #IPL OR #RCB OR #CSK OR #MI OR #DC OR #SRH OR #KKR OR #RR OR #LSG OR #GT OR #PBKS OR @IPL OR @ChennaiIPL OR @imVkohli OR @ChennaiIPL OR @msdhoni OR @BCCI OR @ImRo45 OR Cricket OR IPL2022 OR IPL OR RCB OR CSK OR MI OR DC OR SRH OR KKR OR RR OR LSG OR GT OR PBKS OR IPL OR ChennaiIPL OR imVkohli OR ChennaiIPL OR msdhoni OR BCCI OR ImRo45'\n","    query = f'{keyword} -is:retweet lang:en'\n","    num_of_tweets = 100\n","    # your start and end time for fetching tweets\n","    start_time = '2022-05-18T00:00:00Z'\n","    end_time = '2022-05-24T00:00:00Z'\n","    #connecting to the twitter API using clent and the bearer_token credentials from config.py\n","    client = tweepy.Client(\"AAAAAAAAAAAAAAAAAAAAAGyWZwEAAAAAAYa51qdvaXMLT5VYTa1vMGahYb8%3DOFW2DenxaoxylnltQD8MhvXTtA838sWho7Fjv62SD2m6rQYuWU\",wait_on_rate_limit=True)\n","\n","    #using tweepy paginator to get over 100 last tweets from twitter api\n","    tweets = []\n","    users = []\n","    for tweet in tweepy.Paginator(client.search_recent_tweets,\n","                                    query = query,    \n","                                    start_time=start_time,\n","                                    end_time=end_time,\n","                                    tweet_fields = ['author_id', 'id','created_at', 'public_metrics', 'text', 'source','geo'],\n","                                    user_fields = ['name', 'username', 'location', 'verified', 'description'],\n","                                    place_fields=['geo','name'],\n","                                    expansions=['author_id','geo.place_id'],\n","                                    max_results = num_of_tweets, limit=num_of_pages):\n","        tweets.append(tweet)\n","        #print(dir(tweet))\n","        #users.append(tweet.includes['users'])\n","    return tweets\n","\n","\n","#function to clean the tweets and load them into a DataFrame\n","def tweetsETL(tweets):\n","    \n","    result = []\n","    \n","    #regex function to clean the tweet text from haashtags, mentions and links\n","    def cleanTweets(text):\n","        clean_text = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",text).split())\n","        return clean_text\n","    def getHashTags(text):\n","        regex = \"#(\\w+)\"\n","        return re.findall(regex, str(text))\n","    def getMentions(text):\n","        regex = \"@(\\w+)\"\n","        return re.findall(regex, str(text))\n","    # Function to remove punctuations, links, emojis, and stop words\n","    def preprocessTweets(tweet):\n","        tweet = tweet.lower()  #has to be in place\n","        # Remove urls\n","        tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', tweet, flags=re.MULTILINE)\n","        # Remove user @ references and '#' from tweet\n","        tweet = re.sub(r'\\@\\w+|\\#|\\d+', '', tweet)\n","        # Remove stopwords\n","        tweet_tokens = word_tokenize(tweet)  # convert string to tokens\n","        filtered_words = [w for w in tweet_tokens if w not in stop_words]\n","        filtered_words = [w for w in filtered_words if w not in emojis]\n","        filtered_words = [w for w in filtered_words if w in word_list]\n","        # Remove punctuations\n","        unpunctuated_words = [char for char in filtered_words if char not in string.punctuation]\n","        unpunctuated_words = ' '.join(unpunctuated_words)\n","        return \"\".join(unpunctuated_words)  # join words with a space in between them   \n","    def preprocessTweetsSentiments(tweet):\n","        tweet_tokens = word_tokenize(tweet)\n","        lemmatizer = WordNetLemmatizer() # instatiate an object WordNetLemmatizer Class\n","        lemma_words = [lemmatizer.lemmatize(w) for w in tweet_tokens]\n","        return \" \".join(lemma_words)      \n","    def getPolarity(text):\n","        return TextBlob(text).sentiment.polarity    \n","    def getSubjectivity(text):\n","        return TextBlob(text).sentiment.subjectivity  \n","    \n","    def getAnalysis(score):\n","        if score < 0:\n","            return 'Negative'\n","        elif score == 0:\n","            return 'Neutral'\n","        else:\n","            return 'Positive'\n","    #function to unpack the tweets list into a dataframe\n","    for i in range(0, num_of_pages) :        \n","        for tweet, user in zip(tweets[i].data, tweets[i].includes['users']):\n","                p_tweet=preprocessTweets(tweet.text)\n","                s_tweet=preprocessTweetsSentiments(p_tweet)\n","                result.append({'tweet_id': tweet.id,\n","                               'author_id': tweet.author_id,                               \n","                               'tweet_by':user.username,\n","                               'tweet_by_verified':user.verified,\n","                               'user_location':user.location,\n","                               'text': tweet.text,\n","                               'clean_tweet' : p_tweet,\n","                              #  'adjectives' : getAdjectives(p_tweet),\n","                               'sentiment_text' : s_tweet,\n","                               'created_at': tweet.created_at,\n","                               'source':tweet.source,\n","                               'retweets': tweet.public_metrics['retweet_count'],\n","                               'replies': tweet.public_metrics['reply_count'],\n","                               'likes': tweet.public_metrics['like_count'],\n","                               'quote_count': tweet.public_metrics['quote_count'],\n","                               'hastags':getHashTags(tweet.text),\n","                               'mentions':getMentions(tweet.text),\n","                               'subjectivity':getSubjectivity(s_tweet),\n","                               'polarity':getPolarity(s_tweet),\n","                               'sentiment':getAnalysis(getPolarity(s_tweet))\n","                          })\n","\n","    df = pd.DataFrame(result)     \n","    return df\n","    "]},{"cell_type":"code","execution_count":11,"metadata":{"id":"FjrfKETn8dYh","colab":{"base_uri":"https://localhost:8080/","height":328},"executionInfo":{"status":"error","timestamp":1653467821556,"user_tz":-330,"elapsed":1026,"user":{"displayName":"Himanshu Nigam","userId":"07566813038449216225"}},"outputId":"55f3ae3f-aff0-47a0-cb31-d17282a33bfe"},"outputs":[{"output_type":"error","ename":"BadRequest","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mBadRequest\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-b27804a19680>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtweets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgetTweets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-10-f530405f8a50>\u001b[0m in \u001b[0;36mgetTweets\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m                                     \u001b[0mplace_fields\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'geo'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                                     \u001b[0mexpansions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'author_id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'geo.place_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                                     max_results = num_of_tweets, limit=num_of_pages):\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mtweets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m#print(dir(tweet))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tweepy/pagination.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pagination_token\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpagination_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprevious_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"previous_token\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tweepy/client.py\u001b[0m in \u001b[0;36msearch_recent_tweets\u001b[0;34m(self, query, user_auth, **params)\u001b[0m\n\u001b[1;32m   1253\u001b[0m                 \u001b[0;34m\"since_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sort_order\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"start_time\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tweet.fields\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m                 \u001b[0;34m\"until_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"user.fields\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m             ), data_type=Tweet, user_auth=user_auth\n\u001b[0m\u001b[1;32m   1256\u001b[0m         )\n\u001b[1;32m   1257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tweepy/client.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, method, route, params, endpoint_parameters, json, data_type, user_auth)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         response = self.request(method, route, params=request_params,\n\u001b[0;32m--> 127\u001b[0;31m                                 json=json, user_auth=user_auth)\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_type\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tweepy/client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, route, params, json, user_auth)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mBadRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m401\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mUnauthorized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mBadRequest\u001b[0m: 400 Bad Request\nInvalid 'start_time':'2022-05-18T00:00Z'. 'start_time' must be on or after 2022-05-18T08:37Z"]}],"source":["tweets=getTweets()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3-9Rvm-B8dYj","executionInfo":{"status":"aborted","timestamp":1653467821544,"user_tz":-330,"elapsed":13,"user":{"displayName":"Himanshu Nigam","userId":"07566813038449216225"}}},"outputs":[],"source":["nltk.download('wordnet')\n","nltk.download('averaged_perceptron_tagger')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_-f0i9eh8dYj","executionInfo":{"status":"aborted","timestamp":1653467821546,"user_tz":-330,"elapsed":15,"user":{"displayName":"Himanshu Nigam","userId":"07566813038449216225"}}},"outputs":[],"source":["print(len(tweets))\n","df=tweetsETL(tweets)\n","#df.duplicated(subset='tweet_id').sum()\n","#df=tweets_df.drop_duplicates(subset=['tweet_id']) # drop duplicate values\n","#df.isna().any() # Ch \n","df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"02-9HUVYLXFp","executionInfo":{"status":"aborted","timestamp":1653467821547,"user_tz":-330,"elapsed":16,"user":{"displayName":"Himanshu Nigam","userId":"07566813038449216225"}}},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"-SyWKgrG38kB"},"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MMjkZUkR8dYl","executionInfo":{"status":"aborted","timestamp":1653467821549,"user_tz":-330,"elapsed":18,"user":{"displayName":"Himanshu Nigam","userId":"07566813038449216225"}}},"outputs":[],"source":["df.to_csv('/content/drive/MyDrive/Colab Notebooks/Graph Analysis/Tweets_Processed_Temp3.csv',encoding='utf-8-sig', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VHp7ZofU6Ksb","executionInfo":{"status":"aborted","timestamp":1653467821551,"user_tz":-330,"elapsed":20,"user":{"displayName":"Himanshu Nigam","userId":"07566813038449216225"}}},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uW7CETFa8dYl","executionInfo":{"status":"aborted","timestamp":1653467821553,"user_tz":-330,"elapsed":21,"user":{"displayName":"Himanshu Nigam","userId":"07566813038449216225"}}},"outputs":[],"source":["df.count()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xiOU4bY_0DTp","executionInfo":{"status":"aborted","timestamp":1653467821554,"user_tz":-330,"elapsed":22,"user":{"displayName":"Himanshu Nigam","userId":"07566813038449216225"}}},"outputs":[],"source":["df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ygmu-aBaLNnB","executionInfo":{"status":"aborted","timestamp":1653467821555,"user_tz":-330,"elapsed":23,"user":{"displayName":"Himanshu Nigam","userId":"07566813038449216225"}}},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x5Qxu0VS5kB6","executionInfo":{"status":"aborted","timestamp":1653467821555,"user_tz":-330,"elapsed":23,"user":{"displayName":"Himanshu Nigam","userId":"07566813038449216225"}}},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"ttweet_preprocessing.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"}},"nbformat":4,"nbformat_minor":0}